{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "_<strong>Note:</strong> The course was originally made to be used with GitHub Copilot at the beginning of 2024. As I was doing it at the summer of 2025, where much better alternatives existed, I ended up using PyCharm's JetBrains Assistant, specifically Claude 4 Sonnet._\n",
    "\n",
    "_As the code Clause came up with were much more advanced and gave me often many possible options how to proceed, I sometimes used specific versions of Claude's code manually to follow the original course as close as possible._\n",
    "\n",
    "_I wrote the prompts in almost exactly the same way the course suggested them._"
   ],
   "id": "3f5d1cba45a0ee96"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### _I want to import a dataset named Iris in the python file and store it as a dataframe, can you do it for me?_",
   "id": "c51101670ab9ddef"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-07T15:49:12.339468Z",
     "start_time": "2025-07-07T15:49:12.329315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris_data = load_iris()\n",
    "\n",
    "# Create a DataFrame with the features\n",
    "iris_df = pd.DataFrame(iris_data.data, columns=iris_data.feature_names)\n",
    "\n",
    "# Add the target column (species)\n",
    "iris_df['species'] = iris_data.target\n",
    "\n",
    "# Map target numbers to species names for better readability\n",
    "species_names = {0: 'setosa', 1: 'versicolor', 2: 'virginica'}\n",
    "iris_df['species'] = iris_df['species'].map(species_names)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(\"Dataset shape:\", iris_df.shape)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(iris_df.head())\n",
    "print(\"\\nDataset info:\")\n",
    "print(iris_df.info())"
   ],
   "id": "initial_id",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (150, 5)\n",
      "\n",
      "First few rows:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   sepal length (cm)  150 non-null    float64\n",
      " 1   sepal width (cm)   150 non-null    float64\n",
      " 2   petal length (cm)  150 non-null    float64\n",
      " 3   petal width (cm)   150 non-null    float64\n",
      " 4   species            150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This code will:\n",
    "1. **Import the required libraries**: `pandas` for DataFrame operations and from scikit-learn `load_iris`\n",
    "2. **Load the Iris dataset**: Using `load_iris()` which returns a dictionary-like object\n",
    "3. **Create a DataFrame**: With the feature columns (sepal length, sepal width, petal length, petal width)\n",
    "4. **Add the target column**: The species information\n",
    "5. **Map target values**: Convert numeric targets (0, 1, 2) to readable species names\n",
    "6. **Display dataset information**: Shows the shape, first few rows, and general info about the DataFrame\n",
    "\n",
    "The resulting DataFrame (`iris_df`) will have 150 rows and 5 columns:\n",
    "- `sepal length (cm)`\n",
    "- `sepal width (cm)`\n",
    "- `petal length (cm)`\n",
    "- `petal width (cm)`\n",
    "- `species` (setosa, versicolor, virginica)\n",
    "\n",
    "You can now use this DataFrame for further analysis, visualization, or machine learning tasks!"
   ],
   "id": "15c49ee8a46c3777"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_<strong>Note:</strong> Here GitHub Copilot actually made just pandas import, classic read_csv() function and imported the dataset from GitHub, through an URL. It then map it to specified dataframe and then displayed it through simple df.head() function. As  Claude correctly recognized which datased I am asking him to import, that it is part of sklearn.datasets and made necessary imports directly from its library, made necessary imports and better mapping to specific dataframe, I am leaving only it's version here._",
   "id": "86f565a977d03881"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<strong>Display first 5 rows</strong>",
   "id": "26b2a2fe5bf56023"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:49:17.078178Z",
     "start_time": "2025-07-07T15:49:17.073081Z"
    }
   },
   "cell_type": "code",
   "source": "print(iris_df.head())",
   "id": "9290067ce5de5c98",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "0                5.1               3.5                1.4               0.2   \n",
      "1                4.9               3.0                1.4               0.2   \n",
      "2                4.7               3.2                1.3               0.2   \n",
      "3                4.6               3.1                1.5               0.2   \n",
      "4                5.0               3.6                1.4               0.2   \n",
      "\n",
      "  species  \n",
      "0  setosa  \n",
      "1  setosa  \n",
      "2  setosa  \n",
      "3  setosa  \n",
      "4  setosa  \n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "<strong>Display last 5 rows</strong>",
   "id": "37d743d77b367d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:49:20.630999Z",
     "start_time": "2025-07-07T15:49:20.625749Z"
    }
   },
   "cell_type": "code",
   "source": "print(iris_df.tail())",
   "id": "c499c6148dc288bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
      "145                6.7               3.0                5.2               2.3   \n",
      "146                6.3               2.5                5.0               1.9   \n",
      "147                6.5               3.0                5.2               2.0   \n",
      "148                6.2               3.4                5.4               2.3   \n",
      "149                5.9               3.0                5.1               1.8   \n",
      "\n",
      "       species  \n",
      "145  virginica  \n",
      "146  virginica  \n",
      "147  virginica  \n",
      "148  virginica  \n",
      "149  virginica  \n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "h### _Now in the dataset I want to change the column names for better usability. For example sepal length will become SL only. Can you help me with that?_",
   "id": "ec04d0db8360b24a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:49:25.801574Z",
     "start_time": "2025-07-07T15:49:25.793288Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Rename columns for better usability\n",
    "iris_df.columns = ['SL', 'SW', 'PL', 'PW', 'Species']\n",
    "\n",
    "# Alternative method using rename() function (more explicit)\n",
    "# iris_df = iris_df.rename(columns={\n",
    "#     'sepal length (cm)': 'SL',\n",
    "#     'sepal width (cm)': 'SW',\n",
    "#     'petal length (cm)': 'PL',\n",
    "#     'petal width (cm)': 'PW'\n",
    "# })\n",
    "\n",
    "# Display the updated DataFrame\n",
    "print(\"Updated column names:\")\n",
    "print(iris_df.columns.tolist())\n",
    "print(\"\\nFirst few rows with new column names:\")\n",
    "print(iris_df.head())\n",
    "print(\"\\nDataset info with new column names:\")\n",
    "print(iris_df.info())\n"
   ],
   "id": "7f69eeba188ef453",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated column names:\n",
      "['SL', 'SW', 'PL', 'PW', 'Species']\n",
      "\n",
      "First few rows with new column names:\n",
      "    SL   SW   PL   PW Species\n",
      "0  5.1  3.5  1.4  0.2  setosa\n",
      "1  4.9  3.0  1.4  0.2  setosa\n",
      "2  4.7  3.2  1.3  0.2  setosa\n",
      "3  4.6  3.1  1.5  0.2  setosa\n",
      "4  5.0  3.6  1.4  0.2  setosa\n",
      "\n",
      "Dataset info with new column names:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   SL       150 non-null    float64\n",
      " 1   SW       150 non-null    float64\n",
      " 2   PL       150 non-null    float64\n",
      " 3   PW       150 non-null    float64\n",
      " 4   Species  150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The new column names are:\n",
    "- `SL` - Sepal Length\n",
    "- `SW` - Sepal Width\n",
    "- `PL` - Petal Length\n",
    "- `PW` - Petal Width\n",
    "- `Species` - Species (kept as is for clarity)\n",
    "\n",
    "I've provided two methods:\n",
    "1. **Direct assignment** (used in the main code): Simply assigns new column names in the same order\n",
    "2. **Using rename() method** (commented alternative): More explicit mapping of old names to new names\n",
    "\n",
    "Both methods will give you the same result. The direct assignment method is more concise when you're renaming all columns, while the `rename()` method is better when you only want to rename specific columns.\n",
    "Now your DataFrame will be much easier to work with using these shorter, more convenient column names!"
   ],
   "id": "bdd122994ed4407a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_<strong>Note:</strong> Here Copilot used approach marked as \"alternative\" by Claude. I used the Claude's primary one for this case, but as the alternative one is more explicit with details on how you want the names renamed, in may be in some cases better approach._",
   "id": "2d6bffcb445eaded"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### _Thank you. Now I want to check how many different species are there in my dataframe in the Species column, how do I do it?_",
   "id": "ee633e241b011910"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Great question! There are several ways to check the different species in your DataFrame. Here are the most useful methods:",
   "id": "9b031b092690b9bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:54:01.000562Z",
     "start_time": "2025-07-07T15:54:00.995364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Method 1: Get unique species names\n",
    "print(\"Unique species:\")\n",
    "print(iris_df['Species'].unique())  # Changed from 'species' to 'Species'\n",
    "\n",
    "# Method 2: Count of each species (most informative)\n",
    "print(\"\\nCount of each species:\")\n",
    "print(iris_df['Species'].value_counts())  # Changed from 'species' to 'Species'\n",
    "\n",
    "# Method 3: Number of unique species\n",
    "print(f\"\\nTotal number of different species: {iris_df['Species'].nunique()}\")\n",
    "\n",
    "# Method 4: Get unique species as a sorted list\n",
    "print(\"\\nUnique species (sorted):\")\n",
    "print(sorted(iris_df['Species'].unique()))\n",
    "\n",
    "# Method 5: Display species counts with percentages\n",
    "print(\"\\nSpecies distribution (with percentages):\")\n",
    "print(iris_df['Species'].value_counts(normalize=True) * 100)\n"
   ],
   "id": "6e7b8aa18eb09d10",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique species:\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Count of each species:\n",
      "Species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total number of different species: 3\n",
      "\n",
      "Unique species (sorted):\n",
      "['setosa', 'versicolor', 'virginica']\n",
      "\n",
      "Species distribution (with percentages):\n",
      "Species\n",
      "setosa        33.333333\n",
      "versicolor    33.333333\n",
      "virginica     33.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "This will show you:\n",
    "1. **`.unique()`** - Returns an array of unique species names\n",
    "2. **`.value_counts()`** - Shows each species and how many times it appears (most useful)\n",
    "3. **`.nunique()`** - Returns the total number of unique species\n",
    "4. **`sorted(.unique())`** - Shows unique species in alphabetical order\n",
    "5. **`.value_counts(normalize=True)`** - Shows the percentage distribution of each species\n",
    "\n",
    "The Iris dataset typically contains 3 species:\n",
    "- **setosa** (50 samples)\n",
    "- **versicolor** (50 samples)\n",
    "- **virginica** (50 samples)\n",
    "\n",
    "The `value_counts()` method is usually the most informative as it shows both the species names and their frequencies, which is helpful for understanding the distribution of your data.\n"
   ],
   "id": "5cb6292012b2c770"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_<strong>Note:</strong> Here Copilot simply gave as an answer just a fraction of possible exploration methods compared to Claude. I just used cell as Claude created it - but first it turned out I have to do some bughunting, as the cell above originally returned an error to me. I solved it with the help of Claude rather quickly (the next cell is what returned us what we needed to find where the error lies). When I changed at the beginning the names of the columns, I added one change manually (to more closely reflect appearance of columns in the original course, where the respective column, in my dataframe originally named species, starts with a capital - and as such renamed it to Species. THe Claude's code simply did not reflect the change, as it did not know about it. That is a lesson of itself - when you work on something with the help of AI, you should really include it as a partner. Otherwise, miscomunication failures awaits you.)_",
   "id": "e8db09f433943858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T15:50:54.018745Z",
     "start_time": "2025-07-07T15:50:54.008613Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# First, let's examine the DataFrame structure\n",
    "print(\"DataFrame info:\")\n",
    "print(iris_df.info())\n",
    "print(\"\\nColumn names:\")\n",
    "print(iris_df.columns.tolist())\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(iris_df.head())\n",
    "\n",
    "# Find the target column (usually the last column in iris dataset)\n",
    "# Common names for species column: 'species', 'Species', 'target', 'class', 'variety'\n",
    "possible_target_columns = ['species', 'Species', 'target', 'class', 'variety']\n",
    "target_column = None\n",
    "\n",
    "for col in possible_target_columns:\n",
    "    if col in iris_df.columns:\n",
    "        target_column = col\n",
    "        break\n",
    "\n",
    "# If none of the common names found, assume it's the last column\n",
    "if target_column is None:\n",
    "    target_column = iris_df.columns[-1]\n",
    "    print(f\"Using last column as target: '{target_column}'\")\n",
    "\n",
    "# Method 1: Get unique species names\n",
    "print(\"Unique species:\")\n",
    "print(iris_df[target_column].unique())\n",
    "\n",
    "# Method 2: Count of each species (most informative)\n",
    "print(\"\\nCount of each species:\")\n",
    "print(iris_df[target_column].value_counts())\n",
    "\n",
    "# Method 3: More detailed information\n",
    "print(f\"\\nDetailed info about '{target_column}' column:\")\n",
    "print(f\"Total samples: {len(iris_df)}\")\n",
    "print(f\"Unique values: {iris_df[target_column].nunique()}\")\n",
    "print(\"\\nValue counts with percentages:\")\n",
    "print(iris_df[target_column].value_counts(normalize=True) * 100)\n"
   ],
   "id": "27d64f91528de667",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   SL       150 non-null    float64\n",
      " 1   SW       150 non-null    float64\n",
      " 2   PL       150 non-null    float64\n",
      " 3   PW       150 non-null    float64\n",
      " 4   Species  150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n",
      "\n",
      "Column names:\n",
      "['SL', 'SW', 'PL', 'PW', 'Species']\n",
      "\n",
      "First few rows:\n",
      "    SL   SW   PL   PW Species\n",
      "0  5.1  3.5  1.4  0.2  setosa\n",
      "1  4.9  3.0  1.4  0.2  setosa\n",
      "2  4.7  3.2  1.3  0.2  setosa\n",
      "3  4.6  3.1  1.5  0.2  setosa\n",
      "4  5.0  3.6  1.4  0.2  setosa\n",
      "Unique species:\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "\n",
      "Count of each species:\n",
      "Species\n",
      "setosa        50\n",
      "versicolor    50\n",
      "virginica     50\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Detailed info about 'Species' column:\n",
      "Total samples: 150\n",
      "Unique values: 3\n",
      "\n",
      "Value counts with percentages:\n",
      "Species\n",
      "setosa        33.333333\n",
      "versicolor    33.333333\n",
      "virginica     33.333333\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### _Now I want to perform exploratory data analysis on my dataset, how do I get a quick overview of the dataframe?_",
   "id": "2b5cdf4d4a42d166"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Great question! For exploratory data analysis (EDA), there are several pandas methods that give you a comprehensive overview of your dataset. Here are the most useful ones for getting a quick understanding of your Iris dataset:\n",
   "id": "cd8d50600aff1425"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# 1. Basic information about the dataset\n",
    "print(\"=== DATASET OVERVIEW ===\")\n",
    "print(f\"Dataset shape: {iris_df.shape}\")\n",
    "print(f\"Total samples: {len(iris_df)}\")\n",
    "print(f\"Total features: {len(iris_df.columns)}\")\n",
    "\n",
    "# 2. Column information and data types\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "print(iris_df.info())\n",
    "\n",
    "# 3. Statistical summary of numerical columns\n",
    "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "print(iris_df.describe())\n",
    "\n",
    "# 4. Check for missing values\n",
    "print(\"\\n=== MISSING VALUES ===\")\n",
    "print(iris_df.isnull().sum())\n",
    "\n",
    "# 5. First and last few rows\n",
    "print(\"\\n=== FIRST 5 ROWS ===\")\n",
    "print(iris_df.head())\n",
    "\n",
    "print(\"\\n=== LAST 5 ROWS ===\")\n",
    "print(iris_df.tail())\n",
    "\n",
    "# 6. Unique values in categorical columns\n",
    "print(\"\\n=== CATEGORICAL DATA ANALYSIS ===\")\n",
    "print(f\"Unique species: {iris_df['Species'].unique()}\")\n",
    "print(f\"Species counts:\\n{iris_df['Species'].value_counts()}\")\n",
    "\n",
    "# 7. Data types of each column\n",
    "print(\"\\n=== DATA TYPES ===\")\n",
    "print(iris_df.dtypes)\n",
    "\n",
    "# 8. Memory usage\n",
    "print(\"\\n=== MEMORY USAGE ===\")\n",
    "print(f\"Memory usage: {iris_df.memory_usage(deep=True).sum()} bytes\")\n"
   ],
   "id": "93316480497c7bbd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Absolutely! Let me explain each key EDA method and what it tells you about your Iris dataset:\n",
    "## Key EDA Methods Explained\n",
    "### 1. **`.shape`**\n",
    "- **What it does**: Returns a tuple showing (number of rows, number of columns)\n",
    "- **For your Iris dataset**: Shows `(150, 5)` meaning 150 samples and 5 features\n",
    "- **Why it's useful**: Gives you immediate sense of dataset size and complexity\n",
    "\n",
    "### 2. **`.info()`**\n",
    "- **What it does**: Provides a comprehensive summary including:\n",
    "    - Column names and their data types\n",
    "    - Number of non-null values per column\n",
    "    - Memory usage\n",
    "\n",
    "- **For your Iris dataset**: Shows all 150 entries are complete (no missing data), 4 float64 columns for measurements, 1 object column for species\n",
    "- **Why it's useful**: Instantly reveals data quality issues and column types\n",
    "\n",
    "### 3. **`.describe()`**\n",
    "- **What it does**: Calculates statistical summary for numerical columns:\n",
    "    - Count, mean, standard deviation\n",
    "    - Minimum and maximum values\n",
    "    - 25th, 50th (median), and 75th percentiles\n",
    "\n",
    "- **For your Iris dataset**: Shows ranges and distributions of SL, SW, PL, PW measurements\n",
    "- **Why it's useful**: Helps identify outliers and understand data distribution\n",
    "\n",
    "### 4. **`.isnull().sum()`**\n",
    "- **What it does**: Counts missing (null/NaN) values in each column\n",
    "- **For your Iris dataset**: Should show 0 for all columns (complete dataset)\n",
    "- **Why it's useful**: Critical for data quality assessment - missing data affects analysis\n",
    "\n",
    "### 5. **`.head()` and `.tail()`**\n",
    "- **What it does**: Shows first 5 and last 5 rows respectively\n",
    "- **For your Iris dataset**: Lets you see actual data values and verify correct loading\n",
    "- **Why it's useful**: Visual inspection to spot formatting issues or data patterns\n",
    "\n",
    "### 6. **`.value_counts()`**\n",
    "- **What it does**: Counts frequency of each unique value in categorical columns\n",
    "- **For your Iris dataset**: Shows 50 samples each of setosa, versicolor, virginica\n",
    "- **Why it's useful**: Reveals class distribution and potential imbalance issues\n",
    "\n",
    "### 7. **`.dtypes`**\n",
    "- **What it does**: Shows data type of each column\n",
    "- **For your Iris dataset**: 4 float64 columns (measurements) + 1 object column (species)\n",
    "- **Why it's useful**: Ensures correct data types for analysis (numerical vs categorical)\n",
    "\n",
    "### 8. **`.duplicated().sum()`**\n",
    "- **What it does**: Counts duplicate rows in the dataset\n",
    "- **For your Iris dataset**: Should be 0 (no duplicate samples)\n",
    "- **Why it's useful**: Identifies data collection errors or redundant entries\n",
    "\n",
    "## Why These Methods Matter for Your Iris Dataset:\n",
    "1. **Data Quality**: Confirms your dataset is clean (no missing values, no duplicates)\n",
    "2. **Data Structure**: Shows balanced dataset with equal species representation\n",
    "3. **Data Types**: Verifies measurements are numerical and species is categorical\n",
    "4. **Statistical Insights**: Reveals measurement ranges and distributions\n",
    "5. **Analysis Readiness**: Confirms dataset is ready for machine learning algorithms\n",
    "\n"
   ],
   "id": "3b6c9a56cd2be9cc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "These methods give you confidence in your data quality and help you understand what you're working with before diving into more advanced analysis or modeling!\n",
   "id": "fee6e4e3ce546ab4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T16:11:26.742364Z",
     "start_time": "2025-07-07T16:11:26.732825Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 3. Statistical summary of numerical columns\n",
    "print(\"\\n=== STATISTICAL SUMMARY ===\")\n",
    "print(iris_df.describe())"
   ],
   "id": "47b34e2bb1a58d4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== STATISTICAL SUMMARY ===\n",
      "               SL          SW          PL          PW\n",
      "count  150.000000  150.000000  150.000000  150.000000\n",
      "mean     5.843333    3.057333    3.758000    1.199333\n",
      "std      0.828066    0.435866    1.765298    0.762238\n",
      "min      4.300000    2.000000    1.000000    0.100000\n",
      "25%      5.100000    2.800000    1.600000    0.300000\n",
      "50%      5.800000    3.000000    4.350000    1.300000\n",
      "75%      6.400000    3.300000    5.100000    1.800000\n",
      "max      7.900000    4.400000    6.900000    2.500000\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-07T16:14:29.743592Z",
     "start_time": "2025-07-07T16:14:29.738111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 2. Column information and data types\n",
    "print(\"\\n=== COLUMN INFORMATION ===\")\n",
    "print(iris_df.info())"
   ],
   "id": "f5f7a167b5ef5f7e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COLUMN INFORMATION ===\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   SL       150 non-null    float64\n",
      " 1   SW       150 non-null    float64\n",
      " 2   PL       150 non-null    float64\n",
      " 3   PW       150 non-null    float64\n",
      " 4   Species  150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "_<strong>Note:</strong> Again here Copilot did not give even a fraction of analysis options Claude came up with, not to mention the detailed explanation for them Claude provided - one of the really strong points of Claude as an AI, being a great mentor. The two I used at the end are the two Copilot came up with. And by this we are at the end of the first part of the project, and will continue in second - actual analysis of data using GenAI._\n",
   "id": "7aeb23bc8e001f61"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
